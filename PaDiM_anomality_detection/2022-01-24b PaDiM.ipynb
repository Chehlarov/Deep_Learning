{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2982bc6a",
   "metadata": {},
   "source": [
    "# PaDiM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c47fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/remmarp/PaDiM-TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc78d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f9a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import morphology\n",
    "from skimage.segmentation import mark_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d692af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.spatial.distance import mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce22ef5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# data_loader.py \n",
    "class MVTecADLoader(object):\n",
    "    #     base_path = r'D:\\mvtec_ad'\n",
    "    base_path = r'mvtec_ad'\n",
    "\n",
    "\n",
    "    train, test = None, None\n",
    "    num_train, num_test = 0, 0\n",
    "\n",
    "    category = {'bottle': ['good', 'broken_large', 'broken_small', 'contamination'],\n",
    "                'cable': ['good', 'bent_wire', 'cable_swap', 'combined', 'cut_inner_insulation', 'cut_outer_insulation',\n",
    "                          'missing_cable', 'missing_wire', 'poke_insulation'],\n",
    "                'capsule': ['good', 'crack', 'faulty_imprint', 'poke', 'scratch', 'squeeze'],\n",
    "                'carpet': ['good', 'color', 'cut', 'hole', 'metal_contamination', 'thread'],\n",
    "                'grid': ['good', 'bent', 'broken', 'glue', 'metal_contamination', 'thread'],\n",
    "                'hazelnut': ['good', 'crack', 'cut', 'hole', 'print'],\n",
    "                'leather': ['good', 'color', 'cut', 'fold', 'glue', 'poke'],\n",
    "                'metal_nut': ['good', 'bent', 'color', 'flip', 'scratch'],\n",
    "                'pill': ['good', 'color', 'combined', 'contamination', 'crack', 'faulty_imprint', 'pill_type',\n",
    "                         'scratch'],\n",
    "                'screw': ['good', 'manipulated_front', 'scratch_head', 'scratch_neck', 'thread_side', 'thread_top'],\n",
    "                'tile': ['good', 'crack', 'glue_strip', 'gray_stroke', 'oil', 'rough'],\n",
    "                'toothbrush': ['good', 'defective'],\n",
    "                'transistor': ['good', 'bent_lead', 'cut_lead', 'damaged_case', 'misplaced'],\n",
    "                'wood': ['good', 'color', 'combined', 'good', 'hole', 'liquid', 'scratch'],\n",
    "                'zipper': ['good', 'broken_teeth', 'combined', 'fabric_border', 'fabric_interior', 'rough',\n",
    "                           'split_teeth', 'squeezed_teeth']}\n",
    "\n",
    "    def setup_base_path(self, path):\n",
    "        self.base_path = path\n",
    "\n",
    "    def load(self, category, repeat=4, max_rot=10):\n",
    "        # data, mask, binary anomaly label (0 for anomaly, 1 for good)\n",
    "        x, y, z = [], [], []\n",
    "\n",
    "        # Load train set\n",
    "        path = os.path.join(os.path.join(self.base_path, category), 'train\\good')\n",
    "        files = os.listdir(path)\n",
    "\n",
    "        zero_mask = tf.zeros(shape=(224, 224), dtype=tf.int32)\n",
    "\n",
    "        for rdx in range(repeat):\n",
    "            for _files in files:\n",
    "                full_path = os.path.join(path, _files)\n",
    "                img = self._read_image(full_path=full_path)\n",
    "\n",
    "                if not max_rot == 0:\n",
    "                    img = tf.keras.preprocessing.image.random_rotation(img, max_rot)\n",
    "\n",
    "                mask = zero_mask\n",
    "\n",
    "                x.append(img)\n",
    "                y.append(mask)\n",
    "                z.append(1)\n",
    "\n",
    "        x = np.asarray(x)\n",
    "        y = np.asarray(y)\n",
    "        self.num_train = len(x)\n",
    "\n",
    "        x = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(x, dtype=tf.float32))\n",
    "        y = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(y, dtype=tf.int32))\n",
    "        z = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(z, dtype=tf.int32))\n",
    "\n",
    "        self.train = tf.data.Dataset.zip((x, y, z))\n",
    "\n",
    "        # data, anomaly label (e.g., good, cut, ..., etc.), binary anomaly label (0 for anomaly, 1 for good)\n",
    "        x, y, z = [], [], []\n",
    "\n",
    "        # Load test set\n",
    "        for _label in self.category[category]:\n",
    "            path = os.path.join(os.path.join(self.base_path, category), f'test\\\\{_label}')\n",
    "\n",
    "            files = os.listdir(path)\n",
    "            for _files in files:\n",
    "                full_path = os.path.join(path, _files)\n",
    "                img = self._read_image(full_path=full_path)\n",
    "\n",
    "                if _label == 'good':\n",
    "                    mask = zero_mask\n",
    "                else:\n",
    "                    mask_path = os.path.join(os.path.join(self.base_path, category), 'ground_truth/{}'.format(_label))\n",
    "                    _mask_path = os.path.join(mask_path, '{}_mask.png'.format(_files.split('.')[0]))\n",
    "                    mask = cv2.resize(cv2.imread(_mask_path, flags=cv2.IMREAD_GRAYSCALE), dsize=(256, 256)) / 255\n",
    "                    mask = mask[16:-16, 16:-16]\n",
    "                    mask = tf.convert_to_tensor(mask, dtype=tf.int32)\n",
    "\n",
    "                x.append(img)\n",
    "                y.append(mask)\n",
    "                z.append(int(self.category[category].index(_label) == 0))\n",
    "\n",
    "        x = np.asarray(x)\n",
    "        y = np.asarray(y)\n",
    "        self.num_test = len(x)\n",
    "\n",
    "        x = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(x, dtype=tf.float32))\n",
    "        y = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(y, dtype=tf.int32))\n",
    "        z = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(z, dtype=tf.int32))\n",
    "\n",
    "        self.test = tf.data.Dataset.zip((x, y, z))\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_image(full_path, flags=cv2.IMREAD_COLOR):\n",
    "        img = cv2.imread(full_path, flags=flags)\n",
    "        b, g, r = cv2.split(img)\n",
    "        img = cv2.merge([r, g, b])\n",
    "\n",
    "        img = cv2.resize(img, dsize=(256, 256))\n",
    "\n",
    "        img = img[16:-16, 16:-16, :]\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f9aa20",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# utils.py\n",
    "def embedding_concat(l1, l2):\n",
    "    bs, h1, w1, c1 = l1.shape\n",
    "    _, h2, w2, c2 = l2.shape\n",
    "\n",
    "    s = int(h1 / h2)\n",
    "    x = tf.compat.v1.extract_image_patches(l1, ksizes=[1, s, s, 1], strides=[1, s, s, 1], rates=[1, 1, 1, 1],\n",
    "                                           padding='VALID')\n",
    "    x = tf.reshape(x, (bs, -1, h2, w2, c1))\n",
    "\n",
    "    col_z = []\n",
    "    for idx in range(x.shape[1]):\n",
    "        col_z.append(tf.concat([x[:, idx, :, :, :], l2], axis=-1))\n",
    "    z = tf.stack(col_z, axis=1)\n",
    "\n",
    "    z = tf.reshape(z, (bs, h2, w2, -1))\n",
    "    if s == 1:\n",
    "        return z\n",
    "    z = tf.nn.depth_to_space(z, block_size=s)\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "def plot_fig(test_img, scores, gts, threshold, save_dir, class_name):\n",
    "    num = len(scores)\n",
    "    vmax = scores.max() * 255.\n",
    "    vmin = scores.min() * 255.\n",
    "    for i in range(num):\n",
    "        img = test_img[i][0]\n",
    "\n",
    "        gt = gts[i].transpose(1, 2, 0).squeeze()\n",
    "\n",
    "        heat_map = scores[i] * 255\n",
    "        mask = scores[i]\n",
    "        mask[mask > threshold] = 1\n",
    "        mask[mask <= threshold] = 0\n",
    "\n",
    "        kernel = morphology.disk(4)\n",
    "        mask = morphology.opening(mask, kernel)\n",
    "        mask *= 255\n",
    "        vis_img = mark_boundaries(img, mask, color=(1, 0, 0), mode='thick')\n",
    "        fig_img, ax_img = plt.subplots(1, 5, figsize=(12, 3))\n",
    "        fig_img.subplots_adjust(right=0.9)\n",
    "        norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "        for ax_i in ax_img:\n",
    "            ax_i.axes.xaxis.set_visible(False)\n",
    "            ax_i.axes.yaxis.set_visible(False)\n",
    "        ax_img[0].imshow(img.astype(int))\n",
    "        ax_img[0].title.set_text('Image')\n",
    "        ax_img[1].imshow(gt.astype(int), cmap='gray')\n",
    "        ax_img[1].title.set_text('GroundTruth')\n",
    "        ax = ax_img[2].imshow(heat_map, cmap='jet', norm=norm)\n",
    "        ax_img[2].imshow(img.astype(int), cmap='gray', interpolation='none')\n",
    "        ax_img[2].imshow(heat_map, cmap='jet', alpha=0.5, interpolation='none')\n",
    "        ax_img[2].title.set_text('Predicted heat map')\n",
    "        ax_img[3].imshow(mask.astype(int), cmap='gray')\n",
    "        ax_img[3].title.set_text('Predicted mask')\n",
    "        ax_img[4].imshow(vis_img.astype(int))\n",
    "        ax_img[4].title.set_text('Segmentation result')\n",
    "        left = 0.92\n",
    "        bottom = 0.15\n",
    "        width = 0.015\n",
    "        height = 1 - 2 * bottom\n",
    "        rect = [left, bottom, width, height]\n",
    "        cbar_ax = fig_img.add_axes(rect)\n",
    "        cb = plt.colorbar(ax, shrink=0.6, cax=cbar_ax, fraction=0.046)\n",
    "        cb.ax.tick_params(labelsize=8)\n",
    "        font = {\n",
    "            'family': 'serif',\n",
    "            'color': 'black',\n",
    "            'weight': 'normal',\n",
    "            'size': 8,\n",
    "        }\n",
    "        cb.set_label('Anomaly Score', fontdict=font)\n",
    "\n",
    "        fig_img.savefig(os.path.join(save_dir, class_name + '_{}'.format(i)), dpi=100)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def draw_auc(fp_list, tp_list, auc, path):\n",
    "    plt.figure()\n",
    "    plt.plot(fp_list, tp_list, color='darkorange', lw=2, label='ROC curve (area = {:.4f})'.format(auc))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(path)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def draw_precision_recall(precision, recall, base_line, path):\n",
    "    f1_score = []\n",
    "    for _idx in range(0, len(precision)):\n",
    "        _precision = precision[_idx]\n",
    "        _recall = recall[_idx]\n",
    "\n",
    "        if _precision + _recall == 0:\n",
    "            _f1 = 0\n",
    "        else:\n",
    "            _f1 = 2 * (_precision * _recall) / (_precision + _recall)\n",
    "        f1_score.append(_f1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, marker='.', label='precision-recall curve')\n",
    "    plt.plot([0, 1], [base_line, base_line], linestyle='--', color='grey', label='No skill ({:.04f})'.format(base_line))\n",
    "    plt.plot(recall, f1_score, linestyle='-', color='red', label='f1 score (Max.: {:.4f})'.format(np.max(f1_score)))\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.savefig(path)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    return np.max(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eb58568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor = tf.keras.layers.Input([224, 224, 3], dtype=tf.float32)\n",
    "# x = tf.keras.applications.efficientnet.preprocess_input(input_tensor)\n",
    "# model = tf.keras.applications.EfficientNetB7(include_top=False,\n",
    "#                                              weights='imagenet',\n",
    "#                                              input_tensor=x,\n",
    "#                                              pooling=None)                                \n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "309b2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padim.py \n",
    "def embedding_net(net_type='res'):\n",
    "    input_tensor = tf.keras.layers.Input([224, 224, 3], dtype=tf.float32)\n",
    "\n",
    "    if net_type == 'res':\n",
    "        # resnet 50v2\n",
    "        x = tf.keras.applications.resnet_v2.preprocess_input(input_tensor)\n",
    "        model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_tensor=x, pooling=None)\n",
    "\n",
    "        layer1 = model.get_layer(name='conv3_block1_preact_relu').output\n",
    "        layer2 = model.get_layer(name='conv4_block1_preact_relu').output\n",
    "        layer3 = model.get_layer(name='conv5_block1_preact_relu').output\n",
    "\n",
    "    elif net_type == 'eff':\n",
    "        # efficient net B7\n",
    "        x = tf.keras.applications.efficientnet.preprocess_input(input_tensor)\n",
    "        model = tf.keras.applications.EfficientNetB7(include_top=False, weights='imagenet', input_tensor=x,\n",
    "                                                     pooling=None)\n",
    "\n",
    "        layer1 = model.get_layer(name='block5a_activation').output\n",
    "        layer2 = model.get_layer(name='block6a_activation').output\n",
    "        layer3 = model.get_layer(name='block7a_activation').output\n",
    "        \n",
    "    elif net_type == 'eff_net1':\n",
    "        # efficient net B7; according to github gives best location indication\n",
    "        x = tf.keras.applications.efficientnet.preprocess_input(input_tensor)\n",
    "        model = tf.keras.applications.EfficientNetB7(include_top=False, weights='imagenet', input_tensor=x,\n",
    "                                                     pooling=None)\n",
    "\n",
    "        layer1 = model.get_layer(name='block5a_expand_activation').output\n",
    "        layer2 = model.get_layer(name='block6a_expand_activation').output\n",
    "        layer3 = model.get_layer(name='block7a_expand_activation').output\n",
    "        \n",
    "    elif net_type == 'res_chehlarov':\n",
    "        # new by Chehlarov\n",
    "        x = tf.keras.applications.resnet.preprocess_input(input_tensor)\n",
    "        model = tf.keras.applications.ResNet101(include_top=False, weights='imagenet', input_tensor=x, pooling=None)\n",
    "\n",
    "        layer1 = model.get_layer(name='conv3_block1_preact_relu').output\n",
    "        layer2 = model.get_layer(name='conv4_block1_preact_relu').output\n",
    "        layer3 = model.get_layer(name='conv5_block1_preact_relu').output\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"[NotAllowedNetType] network type is not allowed \")\n",
    "\n",
    "    model.trainable = False\n",
    "    # model.summary(line_length=100)\n",
    "    shape = (layer1.shape[1], layer1.shape[2], layer1.shape[3] + layer2.shape[3] + layer3.shape[3])\n",
    "\n",
    "    return tf.keras.Model(model.input, outputs=[layer1, layer2, layer3]), shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f2d8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padim(category, batch_size, rd, net_type='eff', is_plot=False):\n",
    "    loader = MVTecADLoader()\n",
    "    loader.load(category=category, repeat=1, max_rot=0)\n",
    "\n",
    "    train_set = loader.train.batch(batch_size=batch_size, drop_remainder=True).shuffle(buffer_size=loader.num_train,\n",
    "                                                                                       reshuffle_each_iteration=True)\n",
    "    test_set = loader.test.batch(batch_size=1, drop_remainder=False)\n",
    "\n",
    "    net, _shape = embedding_net(net_type=net_type)\n",
    "    h, w, c = _shape  # height and width of layer1, channel sum of layer 1, 2, and 3, and randomly sampled dimension\n",
    "\n",
    "    out = []\n",
    "    for x, _, _ in train_set:\n",
    "        l1, l2, l3 = net(x)\n",
    "        _out = tf.reshape(embedding_concat(embedding_concat(l1, l2), l3), (batch_size, h * w, c))  # (b, h x w, c)\n",
    "        out.append(_out.numpy())\n",
    "\n",
    "    # calculate multivariate Gaussian distribution.\n",
    "    out = np.concatenate(out, axis=0)\n",
    "    out = np.transpose(out, axes=[0, 2, 1])  # (b, c, h * w)\n",
    "\n",
    "    # RD: random dimension selecting\n",
    "    tmp = tf.unstack(out, axis=0)\n",
    "    _tmp = []\n",
    "    rd_indices = tf.random.shuffle(tf.range(c))[:rd]\n",
    "    for tensor in tmp:\n",
    "        _tmp.append(tf.gather(tensor, rd_indices))\n",
    "    out = tf.stack(_tmp, axis=0)\n",
    "\n",
    "    mu = np.mean(out, axis=0)\n",
    "    cov = np.zeros((rd, rd, h * w))\n",
    "    identity = np.identity(rd)\n",
    "\n",
    "    for idx in range(h * w):\n",
    "        cov[:, :, idx] = np.cov(out[:, :, idx], rowvar=False) + 0.01 * identity\n",
    "\n",
    "    train_outputs = [mu, cov]\n",
    "\n",
    "    out, gt_list, gt_mask, batch_size, test_imgs = [], [], [], 1, []\n",
    "    #  x - data |   y - mask    |   z - binary label\n",
    "    for x, y, z in test_set:\n",
    "        test_imgs.append(x.numpy())\n",
    "        gt_list.append(z.numpy())\n",
    "        gt_mask.append(y.numpy())\n",
    "\n",
    "        l1, l2, l3 = net(x)\n",
    "        _out = tf.reshape(embedding_concat(embedding_concat(l1, l2), l3), (batch_size, h * w, c))  # (BS, h x w, c)\n",
    "        out.append(_out.numpy())\n",
    "\n",
    "    # calculate multivariate Gaussian distribution. skip random dimension selecting\n",
    "    out = np.concatenate(out, axis=0)\n",
    "    gt_list = np.concatenate(gt_list, axis=0)\n",
    "    out = np.transpose(out, axes=[0, 2, 1])\n",
    "\n",
    "    # RD\n",
    "    tmp = tf.unstack(out, axis=0)\n",
    "    _tmp = []\n",
    "    for tensor in tmp:\n",
    "        _tmp.append(tf.gather(tensor, rd_indices)) # Chehlarov: why random - comments above say the opposite\n",
    "    out = tf.stack(_tmp, axis=0)\n",
    "\n",
    "    b, _, _ = out.shape\n",
    "\n",
    "    dist_list = []\n",
    "    for idx in range(h * w):\n",
    "        mu = train_outputs[0][:, idx]\n",
    "        cov_inv = np.linalg.inv(train_outputs[1][:, :, idx])\n",
    "        dist = [mahalanobis(sample[:, idx], mu, cov_inv) for sample in out]\n",
    "        dist_list.append(dist)\n",
    "\n",
    "    dist_list = np.reshape(np.transpose(np.asarray(dist_list), axes=[1, 0]), (b, h, w))\n",
    "\n",
    "    ################\n",
    "    #   DATA Level #\n",
    "    ################\n",
    "    # upsample\n",
    "    score_map = tf.squeeze(tf.image.resize(np.expand_dims(dist_list, -1), size=[h, w])).numpy()\n",
    "\n",
    "    for i in range(score_map.shape[0]):\n",
    "        score_map[i] = gaussian_filter(score_map[i], sigma=4)\n",
    "\n",
    "    # Normalization\n",
    "    max_score = score_map.max()\n",
    "    min_score = score_map.min()\n",
    "    scores = (score_map - min_score) / (max_score - min_score)\n",
    "    scores = -scores\n",
    "\n",
    "    # calculate image-level ROC AUC score\n",
    "    img_scores = scores.reshape(scores.shape[0], -1).max(axis=1)\n",
    "\n",
    "    gt_list = np.asarray(gt_list)\n",
    "    img_roc_auc = metrics.roc_auc_score(gt_list, img_scores)\n",
    "\n",
    "    if is_plot is True:\n",
    "        fpr, tpr, _ = metrics.roc_curve(gt_list, img_scores)\n",
    "        precision, recall, _ = metrics.precision_recall_curve(gt_list, img_scores)\n",
    "\n",
    "        save_dir = os.path.join(os.getcwd(), 'img')\n",
    "        if os.path.isdir(save_dir) is False:\n",
    "            os.mkdir(save_dir)\n",
    "        draw_auc(fpr, tpr, img_roc_auc, os.path.join(save_dir, 'AUROC-{}.png'.format(category)))\n",
    "        base_line = np.sum(gt_list) / len(gt_list)\n",
    "        draw_precision_recall(precision, recall, base_line, os.path.join(os.path.join(save_dir,\n",
    "                                                                                      'PR-{}.png'.format(category))))\n",
    "\n",
    "    #################\n",
    "    #   PATCH Level #\n",
    "    #################\n",
    "    # upsample\n",
    "    score_map = tf.squeeze(tf.image.resize(np.expand_dims(dist_list, -1), size=[224, 224])).numpy()\n",
    "\n",
    "    for i in range(score_map.shape[0]):\n",
    "        score_map[i] = gaussian_filter(score_map[i], sigma=4)\n",
    "\n",
    "    # Normalization\n",
    "    max_score = score_map.max()\n",
    "    min_score = score_map.min()\n",
    "    scores = (score_map - min_score) / (max_score - min_score)\n",
    "    # Note that Binary mask indicates 0 for good and 1 for anomaly. It is opposite from our setting.\n",
    "    # scores = -scores\n",
    "\n",
    "    # calculate per-pixel level ROCAUC\n",
    "    gt_mask = np.asarray(gt_mask)\n",
    "    fp_list, tp_list, _ = metrics.roc_curve(gt_mask.flatten(), scores.flatten())\n",
    "    patch_auc = metrics.auc(fp_list, tp_list)\n",
    "\n",
    "    precision, recall, threshold = metrics.precision_recall_curve(gt_mask.flatten(), scores.flatten(), pos_label=1)\n",
    "    numerator = 2 * precision * recall\n",
    "    denominator = precision + recall\n",
    "\n",
    "    numerator[np.where(denominator == 0)] = 0\n",
    "    denominator[np.where(denominator == 0)] = 1\n",
    "\n",
    "    # get optimal threshold\n",
    "    f1_list = numerator / denominator\n",
    "    best_ths = threshold[np.argmax(f1_list).astype(int)]\n",
    "\n",
    "    print('[{}] image ROCAUC: {:.04f}\\t pixel ROCAUC: {:.04f}'.format(category, img_roc_auc, patch_auc))\n",
    "\n",
    "    if is_plot is True:\n",
    "        save_dir = os.path.join(os.getcwd(), 'img')\n",
    "        if os.path.isdir(save_dir) is False:\n",
    "            os.mkdir(save_dir)\n",
    "        plot_fig(test_imgs, scores, gt_mask, best_ths, save_dir, category)\n",
    "\n",
    "    return img_roc_auc, patch_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36957717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[carpet] image ROCAUC: 0.9446\t pixel ROCAUC: 0.9720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9446227929373997, 0.9720316032862548)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# random.seed(10)\n",
    "tf.random.set_seed(10)\n",
    "padim(category='carpet', batch_size=2, rd=400, net_type='eff_net1', is_plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
